{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/work/saved_data/Global-Canopy-Height-Map\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  g5  g10  g15  g20  g25  g30  totals    longitudes  \\\n",
      "0           0   0    0    0    0    0    0      64  7.406728e+06   \n",
      "1           1   3    0    0    0    0    0       3  4.268613e+06   \n",
      "2           2   0    0    0    0    0    0       1  4.275268e+06   \n",
      "3           3  15    0    0    0    0    0      16  3.915071e+06   \n",
      "4           4   8    0    0    0    0    0      56  3.945878e+06   \n",
      "\n",
      "       latitudes                                         transforms  utm  \\\n",
      "0  296618.124489  | 10.00, 0.00, 293870.00|\\n| 0.00,-10.00, 7406...  39S   \n",
      "1  302341.260358  | 10.00, 0.00, 297320.00|\\n| 0.00,-10.00, 4268...  37N   \n",
      "2  278279.291022  | 10.00, 0.00, 277210.00|\\n| 0.00,-10.00, 4278...  37N   \n",
      "3  544629.566470  | 10.00, 0.00, 540170.00|\\n| 0.00,-10.00, 3915...  37N   \n",
      "4  510736.445518  | 10.00, 0.00, 510730.00|\\n| 0.00,-10.00, 3948...  37N   \n",
      "\n",
      "   zone_number hemisphere   epsg        lat       long  \\\n",
      "0           39          S  32739 -23.436720  49.009242   \n",
      "1           37          N  32637  38.543988  36.731970   \n",
      "2           37          N  32637  38.598237  36.453995   \n",
      "3           37          N  32637  35.377975  39.491350   \n",
      "4           37          N  32637  35.656692  39.118613   \n",
      "\n",
      "                                                path  \n",
      "0  sentinel_1_2_worldwide_utm_39S_30-0000100352-0...  \n",
      "1  sentinel_1_2_worldwide_utm_37N_26-0000075264-0...  \n",
      "2  sentinel_1_2_worldwide_utm_37N_26-0000075264-0...  \n",
      "3  sentinel_1_2_worldwide_utm_37N_26-0000112896-0...  \n",
      "4  sentinel_1_2_worldwide_utm_37N_26-0000112896-0...  \n"
     ]
    }
   ],
   "source": [
    "#samples = pd.read_csv(os.path.join(cwd, 'samples.csv'))\n",
    "samples = pd.read_csv('/home/ubuntu/work/satellite_data/sentinel_pauls_paper/samples.csv')\n",
    "print(samples.head())\n",
    "#cwd = '/home/ubuntu/work/satellite_data/sentinel_pauls_paper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "# Load CSV file\n",
    "#csv_path = os.path.join(cwd, 'samples_2_.csv')\n",
    "csv_path = '/home/ubuntu/work/satellite_data/sentinel_pauls_paper/samples.csv'\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "assert 'path' in data.columns, \"CSV must have a 'path' column\"\n",
    "assert 'latitudes' in data.columns and 'longitudes' in data.columns, \"CSV must have 'latitudes' and 'longitudes' columns for geo splitting\"\n",
    "\n",
    "# Prepend the additional path to the file paths\n",
    "data['path'] = '/home/ubuntu/work/satellite_data/sentinel_pauls_paper/samples/' + data['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataframe.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a subset of the original dataset\n",
    "def create_subset(data, subset_size, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly selects a subset of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): Original dataset as a pandas DataFrame.\n",
    "        subset_size (int): Number of elements to sample.\n",
    "        seed (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A randomly sampled subset of the dataset.\n",
    "    \"\"\"\n",
    "    return data.sample(n=subset_size, random_state=seed).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define splitting functions\n",
    "def split_data_torch(dataset, split_ratios=(0.8, 0.1, 0.1), sample_counts=None, generator=None):\n",
    "    \"\"\"\n",
    "    Splits the data into train, val, and fix_val subsets using torch.utils.data.random_split.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset): The PyTorch dataset.\n",
    "        split_ratios (tuple): Ratios for train, val, and fix_val splits.\n",
    "        sample_counts (tuple): Specific number of samples for train, val, and fix_val splits.\n",
    "        generator (torch.Generator): A torch random generator for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing train, val, and fix_val subsets.\n",
    "    \"\"\"\n",
    "    if sample_counts:\n",
    "        lengths = sample_counts\n",
    "    else:\n",
    "        total_length = len(dataset)\n",
    "        #total_length = sum(sample_counts)\n",
    "        lengths = [int(r * total_length) for r in split_ratios]\n",
    "        lengths[-1] = total_length - sum(lengths[:-1])  # Adjust for rounding errors\n",
    "\n",
    "    train, val, fix_val = random_split(dataset, lengths, generator=generator)\n",
    "    return {'train': train, 'val': val, 'fix_val': fix_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "split_ratios = (0.8, 0.1, 0.1)  # For 80% train, 10% val, 10% fix_val\n",
    "#sample_counts = (100, 20, 20)  # Specific number of samples\n",
    "\n",
    "# Define the subset size\n",
    "#subset_size = sum(sample_counts)  # Example: Use 140 elements from the original dataset\n",
    "#data_subset = create_subset(data, subset_size)\n",
    "data_subset = create_subset(data, len(data))\n",
    "\n",
    "# Convert the DataFrame into a PyTorch Dataset\n",
    "dataset = CustomDataset(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random generator for reproducibility\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Random split with ratios\n",
    "splits_random = split_data_torch(dataset, split_ratios=split_ratios, generator=generator)\n",
    "\n",
    "# Specific count split\n",
    "#splits_count = split_data_torch(dataset, sample_counts=sample_counts, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot index by location index with a non-integer key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save splits to CSV files\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdata_subset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplits_random\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m data_subset\u001b[38;5;241m.\u001b[39miloc[splits_random[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m data_subset\u001b[38;5;241m.\u001b[39miloc[splits_random[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfix_val\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfix_val.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/work/saved_data/Global-Canopy-Height-Map/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/saved_data/Global-Canopy-Height-Map/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1749\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1747\u001b[0m key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(key):\n\u001b[0;32m-> 1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot index by location index with a non-integer key"
     ]
    }
   ],
   "source": [
    "# Save splits to CSV files\n",
    "data_subset.iloc[splits_random['train'].indices].to_csv('train.csv', index=False)\n",
    "data_subset.iloc[splits_random['val'].indices].to_csv('val.csv', index=False)\n",
    "data_subset.iloc[splits_random['fix_val'].indices].to_csv('fix_val.csv', index=False)\n",
    "\n",
    "# Save subsets to CSV\n",
    "#data_subset.iloc[splits_count['train'].indices].to_csv('train.csv', index=False)\n",
    "#data_subset.iloc[splits_count['val'].indices].to_csv('val.csv', index=False)\n",
    "#data_subset.iloc[splits_count['fix_val'].indices].to_csv('fix_val.csv', index=False)\n",
    "\n",
    "print(\"Data saved to train.csv, val.csv, and fix_val.csv.\")\n",
    "\n",
    "'''\n",
    "data.iloc[[idx for idx, _ in splits_count['train']]].to_csv('train.csv', index=False)\n",
    "data.iloc[[idx for idx, _ in splits_count['val']]].to_csv('val.csv', index=False)\n",
    "data.iloc[[idx for idx, _ in splits_count['fix_val']]].to_csv('fix_val.csv', index=False)\n",
    "\n",
    "print(\"Data splitting completed and saved.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
